{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hX4n9TsbGw-f"
   },
   "source": [
    "##### Copyright 2018 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2021-03-06T02:21:53.597904Z",
     "iopub.status.busy": "2021-03-06T02:21:53.597261Z",
     "iopub.status.idle": "2021-03-06T02:21:53.599668Z",
     "shell.execute_reply": "2021-03-06T02:21:53.599099Z"
    },
    "id": "0nbI5DtDGw-i"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TnJztDZGw-n"
   },
   "source": [
    "# Text classification with an RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfN3bMR5Gw-o"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/text/text_classification_rnn\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/text_classification_rnn.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/text_classification_rnn.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/text/text_classification_rnn.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUWearf0Gw-p"
   },
   "source": [
    "This text classification tutorial trains a [recurrent neural network](https://developers.google.com/machine-learning/glossary/#recurrent_neural_network) on the [IMDB large movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/) for sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2VQo4bajwUU"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T02:21:53.611364Z",
     "iopub.status.busy": "2021-03-06T02:21:53.608389Z",
     "iopub.status.idle": "2021-03-06T02:21:56.718039Z",
     "shell.execute_reply": "2021-03-06T02:21:56.717432Z"
    },
    "id": "vH_FAfIz5dEw"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U tensorflow\n",
    "!pip install -q tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T02:21:56.722649Z",
     "iopub.status.busy": "2021-03-06T02:21:56.722055Z",
     "iopub.status.idle": "2021-03-06T02:22:04.565765Z",
     "shell.execute_reply": "2021-03-06T02:22:04.566210Z"
    },
    "id": "z682XYsrjkY9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rXHa-w9JZhb"
   },
   "source": [
    "Import `matplotlib` and create a helper function to plot graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T02:22:04.571812Z",
     "iopub.status.busy": "2021-03-06T02:22:04.571118Z",
     "iopub.status.idle": "2021-03-06T02:22:04.573079Z",
     "shell.execute_reply": "2021-03-06T02:22:04.573442Z"
    },
    "id": "Mp1Z7P9pYRSK"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRmMubr0jrE2"
   },
   "source": [
    "## Setup input pipeline\n",
    "\n",
    "\n",
    "The IMDB large movie review dataset is a *binary classification* datasetâ€”all the reviews have either a *positive* or *negative* sentiment.\n",
    "\n",
    "Download the dataset using [TFDS](https://www.tensorflow.org/datasets). See the [loading text tutorial](../load_data/text.ipynb) for details on how to load this sort of data manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T02:22:04.577855Z",
     "iopub.status.busy": "2021-03-06T02:22:04.577244Z",
     "iopub.status.idle": "2021-03-06T02:22:16.450147Z",
     "shell.execute_reply": "2021-03-06T02:22:16.450588Z"
    },
    "id": "SHRwRoP2nVHX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset, info = tfds.load('imdb_reviews', with_info=True,\n",
    "                          as_supervised=True)\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
    "\n",
    "train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWA4c2ir7g6p"
   },
   "source": [
    "Initially this returns a dataset of (text, label pairs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T02:22:16.456371Z",
     "iopub.status.busy": "2021-03-06T02:22:16.454639Z",
     "iopub.status.idle": "2021-03-06T02:22:17.986883Z",
     "shell.execute_reply": "2021-03-06T02:22:17.986415Z"
    },
    "id": "vd4_BGKyurao"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n",
      "label:  0\n"
     ]
    }
   ],
   "source": [
    "for example, label in train_dataset.take(1):\n",
    "  print('text: ', example.numpy())\n",
    "  print('label: ', label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2qVJzcEluH_"
   },
   "source": [
    "Next shuffle the data for training and create batches of these `(text, label)` pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T02:22:17.991433Z",
     "iopub.status.busy": "2021-03-06T02:22:17.990554Z",
     "iopub.status.idle": "2021-03-06T02:22:17.992886Z",
     "shell.execute_reply": "2021-03-06T02:22:17.993343Z"
    },
    "id": "dDsCaZCDYZgm"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T02:22:17.997502Z",
     "iopub.status.busy": "2021-03-06T02:22:17.996933Z",
     "iopub.status.idle": "2021-03-06T02:22:18.000572Z",
     "shell.execute_reply": "2021-03-06T02:22:18.000942Z"
    },
    "id": "VznrltNOnUc5"
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T02:22:18.005309Z",
     "iopub.status.busy": "2021-03-06T02:22:18.004698Z",
     "iopub.status.idle": "2021-03-06T02:22:19.734040Z",
     "shell.execute_reply": "2021-03-06T02:22:19.734517Z"
    },
    "id": "jqkvdcFv41wC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts:  [b'As a physics student, I\\'ve become aware of many idiot professors, and other so-called experts, in the field. As I continue with my studies, I learn more and more about real physics experiments going on, and about the people who are doing things right.<br /><br />Then, my friends tell me of this \"physics movie\" they want to see. Knowing nothing of it, I\\'m excited, hoping that the information will be presented well.<br /><br />I\\'ve done REAL quantum mechanics; this wasn\\'t it.<br /><br />This movie starts with the basic assumption that anything that occurs to a subatomic particle can, and will, occur to you, if you just open your eyes. Let\\'s think about that, for just a moment.<br /><br />Our bodies are composed of somewhere around 10^30 such subatomic particles. That is a million billion billion billion particles! The more \"mysterious\" quantum effects of just two particles can have a 50% probability of cancelling each other out completely. As you add more and more particles into the mix, it becomes almost impossible to have a large net quantum result. To tell us to believe that this is a valid assumption, with no rationality behind it...it\\'s just stupid.<br /><br />My friend, also in physics, and I counted 3 facts during the course of this movie. But they were presented in the most misleading manner I\\'ve EVER SEEN.<br /><br />I cannot say as much for the neural portion of the movie, as I have not had any kind of medical training. It seemed as though it might have had a slight bit more truth to it, remembering my days in biology, but I cannot say.<br /><br />At least this film had a redeeming quality: the dancing peptides (or whatever they actually were) scene. Not to ruin the invaluable plot that drives this movie, but the main character goes to a wedding, where she sees all different types of personalities \"driven\" by their peptides*, and then the film cuts to the dance floor, where we are spliced between people dancing, sometimes surrounded by CG peptides, and a fully CG scene, filled with dancing peptides. The film, at that point, was trying to tell us how we\\'re \"addicted to emotions,\" so we\\'re treated to the full song of that smash hit, \"Addicted to Love.\"<br /><br />This scene was redeeming, because anyone who could go through THAT scene, and still take this movie seriously...well, you are the ones that need to \"open your eyes.\"'\n",
      " b\"Was the script more fitting for a 30 minute sitcom? Yes, but they still make it work! I thought the actors did a fantastic job with an otherwise bland script, especially Jack Black and Christopher Walken. Most people on the board seem to really hate this film. I personally can't see how that could be, but Envy is just one of those film that you either love it or hate it. Much like Napoleon Dynamite and every Leslie Neilsen movie ever made. You either think it's one of the worst movies ever made or one of the funniest. Don't avoid this movie because of the reviews. Watch it and see if you're one of the ones who really like it! If you do, I guarantee it's worth your money. If you don't like it... well, now you know.\"\n",
      " b'okay, this movie f*ck in\\' rules. it is without question one of the most technically inept pieces of cinema ever made. absolutely terrible, but you GOTTA see it. rent this with your buddies and come up with a drinking game or just have fun, it\\'s hilarious. and the behind-the-scenes featurette proves it, you can do anything with paper plates and finger paint. awesome. okay, rent it just for this one scene: two characters are actually WALKING IN PLACE for about 3 minutes in a shot. the director (on the commentary) says \"yeah, the tracking was so smooth it looks like they\\'re...\". yeah, right man, they are totally walking in place. it\\'s so funny.']\n",
      "\n",
      "labels:  [0 1 0]\n"
     ]
    }
   ],
   "source": [
    "for example, label in train_dataset.take(1):\n",
    "  print('texts: ', example.numpy()[:3])\n",
    "  print()\n",
    "  print('labels: ', label.numpy()[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5eWCo88voPY"
   },
   "source": [
    "## Create the text encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFevcItw15P_"
   },
   "source": [
    "The raw text loaded by `tfds` needs to be processed before it can be used in a model. The simplest way to process text for training is using the `experimental.preprocessing.TextVectorization` layer. This layer has many capabilities, but this tutorial sticks to the default behavior.\n",
    "\n",
    "Create the layer, and pass the dataset's text to the layer's `.adapt` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T02:22:19.740102Z",
     "iopub.status.busy": "2021-03-06T02:22:19.739417Z",
     "iopub.status.idle": "2021-03-06T02:22:23.783418Z",
     "shell.execute_reply": "2021-03-06T02:22:23.782721Z"
    },
    "id": "uC25Lu1Yvuqy"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE=1000\n",
    "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(train_dataset.map(lambda text, label: text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuQzVBbe3Ldu"
   },
   "source": [
    "The `.adapt` method sets the layer's vocabulary. Here are the first 20 tokens. After the padding and unknown tokens they're sorted by frequency: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T02:22:23.788757Z",
     "iopub.status.busy": "2021-03-06T02:22:23.787974Z",
     "iopub.status.idle": "2021-03-06T02:22:23.793254Z",
     "shell.execute_reply": "2021-03-06T02:22:23.792768Z"
    },
    "id": "tBoyjjWg0Ac9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i',\n",
       "       'this', 'that', 'br', 'was', 'as', 'for', 'with', 'movie', 'but'],\n",
       "      dtype='<U14')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = np.array(encoder.get_vocabulary())\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjId5pua3jHQ"
   },
   "source": [
    "Once the vocabulary is set, the layer can encode text into indices. The tensors of indices are 0-padded to the longest sequence in the batch (unless you set a fixed `output_sequence_length`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T02:22:23.797637Z",
     "iopub.status.busy": "2021-03-06T02:22:23.796997Z",
     "iopub.status.idle": "2021-03-06T02:22:23.811497Z",
     "shell.execute_reply": "2021-03-06T02:22:23.811954Z"
    },
    "id": "RGc7C9WiwRWs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 15,   4,   1, ...,   0,   0,   0],\n",
       "       [ 14,   2, 225, ...,   0,   0,   0],\n",
       "       [869,  11,  18, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_example = encoder(example)[:3].numpy()\n",
    "encoded_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5cjz0bS39IN"
   },
   "source": [
    "With the default settings, the process is not completely reversible. There are three main reasons for that:\n",
    "\n",
    "1. The default value for `preprocessing.TextVectorization`'s `standardize` argument is `\"lower_and_strip_punctuation\"`.\n",
    "2. The limited vocabulary size and lack of character-based fallback results in some unknown tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T02:22:23.816747Z",
     "iopub.status.busy": "2021-03-06T02:22:23.816082Z",
     "iopub.status.idle": "2021-03-06T02:22:23.821583Z",
     "shell.execute_reply": "2021-03-06T02:22:23.822014Z"
    },
    "id": "N_tD0QY5wXaK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  b'As a physics student, I\\'ve become aware of many idiot professors, and other so-called experts, in the field. As I continue with my studies, I learn more and more about real physics experiments going on, and about the people who are doing things right.<br /><br />Then, my friends tell me of this \"physics movie\" they want to see. Knowing nothing of it, I\\'m excited, hoping that the information will be presented well.<br /><br />I\\'ve done REAL quantum mechanics; this wasn\\'t it.<br /><br />This movie starts with the basic assumption that anything that occurs to a subatomic particle can, and will, occur to you, if you just open your eyes. Let\\'s think about that, for just a moment.<br /><br />Our bodies are composed of somewhere around 10^30 such subatomic particles. That is a million billion billion billion particles! The more \"mysterious\" quantum effects of just two particles can have a 50% probability of cancelling each other out completely. As you add more and more particles into the mix, it becomes almost impossible to have a large net quantum result. To tell us to believe that this is a valid assumption, with no rationality behind it...it\\'s just stupid.<br /><br />My friend, also in physics, and I counted 3 facts during the course of this movie. But they were presented in the most misleading manner I\\'ve EVER SEEN.<br /><br />I cannot say as much for the neural portion of the movie, as I have not had any kind of medical training. It seemed as though it might have had a slight bit more truth to it, remembering my days in biology, but I cannot say.<br /><br />At least this film had a redeeming quality: the dancing peptides (or whatever they actually were) scene. Not to ruin the invaluable plot that drives this movie, but the main character goes to a wedding, where she sees all different types of personalities \"driven\" by their peptides*, and then the film cuts to the dance floor, where we are spliced between people dancing, sometimes surrounded by CG peptides, and a fully CG scene, filled with dancing peptides. The film, at that point, was trying to tell us how we\\'re \"addicted to emotions,\" so we\\'re treated to the full song of that smash hit, \"Addicted to Love.\"<br /><br />This scene was redeeming, because anyone who could go through THAT scene, and still take this movie seriously...well, you are the ones that need to \"open your eyes.\"'\n",
      "Round-trip:  as a [UNK] [UNK] ive become [UNK] of many [UNK] [UNK] and other [UNK] [UNK] in the [UNK] as i [UNK] with my [UNK] i learn more and more about real [UNK] [UNK] going on and about the people who are doing things [UNK] br then my friends tell me of this [UNK] movie they want to see [UNK] nothing of it im [UNK] [UNK] that the [UNK] will be [UNK] [UNK] br ive done real [UNK] [UNK] this wasnt itbr br this movie starts with the [UNK] [UNK] that anything that [UNK] to a [UNK] [UNK] can and will [UNK] to you if you just open your eyes lets think about that for just a [UNK] br our [UNK] are [UNK] of [UNK] around [UNK] such [UNK] [UNK] that is a [UNK] [UNK] [UNK] [UNK] [UNK] the more [UNK] [UNK] effects of just two [UNK] can have a [UNK] [UNK] of [UNK] each other out completely as you add more and more [UNK] into the [UNK] it becomes almost [UNK] to have a [UNK] [UNK] [UNK] result to tell us to believe that this is a [UNK] [UNK] with no [UNK] behind [UNK] just [UNK] br my friend also in [UNK] and i [UNK] 3 [UNK] during the course of this movie but they were [UNK] in the most [UNK] [UNK] ive ever [UNK] br i cannot say as much for the [UNK] [UNK] of the movie as i have not had any kind of [UNK] [UNK] it seemed as though it might have had a [UNK] bit more truth to it [UNK] my days in [UNK] but i cannot [UNK] br at least this film had a [UNK] quality the [UNK] [UNK] or whatever they actually were scene not to [UNK] the [UNK] plot that [UNK] this movie but the main character goes to a [UNK] where she [UNK] all different [UNK] of [UNK] [UNK] by their [UNK] and then the film [UNK] to the dance [UNK] where we are [UNK] between people [UNK] sometimes [UNK] by [UNK] [UNK] and a [UNK] [UNK] scene [UNK] with [UNK] [UNK] the film at that point was trying to tell us how were [UNK] to [UNK] so were [UNK] to the full song of that [UNK] hit [UNK] to [UNK] br this scene was [UNK] because anyone who could go through that scene and still take this movie [UNK] you are the ones that need to open your eyes                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
      "\n",
      "Original:  b\"Was the script more fitting for a 30 minute sitcom? Yes, but they still make it work! I thought the actors did a fantastic job with an otherwise bland script, especially Jack Black and Christopher Walken. Most people on the board seem to really hate this film. I personally can't see how that could be, but Envy is just one of those film that you either love it or hate it. Much like Napoleon Dynamite and every Leslie Neilsen movie ever made. You either think it's one of the worst movies ever made or one of the funniest. Don't avoid this movie because of the reviews. Watch it and see if you're one of the ones who really like it! If you do, I guarantee it's worth your money. If you don't like it... well, now you know.\"\n",
      "Round-trip:  was the script more [UNK] for a [UNK] minute [UNK] yes but they still make it work i thought the actors did a fantastic job with an otherwise [UNK] script especially jack black and [UNK] [UNK] most people on the [UNK] seem to really hate this film i [UNK] cant see how that could be but [UNK] is just one of those film that you either love it or hate it much like [UNK] [UNK] and every [UNK] [UNK] movie ever made you either think its one of the worst movies ever made or one of the [UNK] dont avoid this movie because of the reviews watch it and see if youre one of the ones who really like it if you do i [UNK] its worth your money if you dont like it well now you know                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
      "\n",
      "Original:  b'okay, this movie f*ck in\\' rules. it is without question one of the most technically inept pieces of cinema ever made. absolutely terrible, but you GOTTA see it. rent this with your buddies and come up with a drinking game or just have fun, it\\'s hilarious. and the behind-the-scenes featurette proves it, you can do anything with paper plates and finger paint. awesome. okay, rent it just for this one scene: two characters are actually WALKING IN PLACE for about 3 minutes in a shot. the director (on the commentary) says \"yeah, the tracking was so smooth it looks like they\\'re...\". yeah, right man, they are totally walking in place. it\\'s so funny.'\n",
      "Round-trip:  okay this movie [UNK] in [UNK] it is without question one of the most [UNK] [UNK] [UNK] of cinema ever made absolutely terrible but you [UNK] see it rent this with your [UNK] and come up with a [UNK] game or just have fun its hilarious and the [UNK] [UNK] [UNK] it you can do anything with [UNK] [UNK] and [UNK] [UNK] [UNK] okay rent it just for this one scene two characters are actually [UNK] in place for about 3 minutes in a shot the director on the [UNK] says [UNK] the [UNK] was so [UNK] it looks like theyre [UNK] right man they are totally [UNK] in place its so funny                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(3):\n",
    "  print(\"Original: \", example[n].numpy())\n",
    "  print(\"Round-trip: \", \" \".join(vocab[encoded_example[n]]))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjUqGVBxGw-t"
   },
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7zsmInBOCPO"
   },
   "source": [
    "![A drawing of the information flow in the model](images/bidirectional.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgs6nnSTGw-t"
   },
   "source": [
    "Above is a diagram of the model. \n",
    "\n",
    "1. This model can be build as a `tf.keras.Sequential`.\n",
    "\n",
    "1. The first layer is the `encoder`, which converts the text to a sequence of token indices.\n",
    "\n",
    "2. After the encoder is an embedding layer. An embedding layer stores one vector per word. When called, it converts the sequences of word indices to sequences of vectors. These vectors are trainable. After training (on enough data), words with similar meanings often have similar vectors.\n",
    "\n",
    "  This index-lookup is much more efficient than the equivalent operation of passing a one-hot encoded vector through a `tf.keras.layers.Dense` layer.\n",
    "\n",
    "3. A recurrent neural network (RNN) processes sequence input by iterating through the elements. RNNs pass the outputs from one timestep to their input on the next timestep.\n",
    "\n",
    "  The `tf.keras.layers.Bidirectional` wrapper can also be used with an RNN layer. This propagates the input forward and backwards through the RNN layer and then concatenates the final output. \n",
    "\n",
    "  * The main advantage to a bidirectional RNN is that the signal from the beginning of the input doesn't need to be processed all the way through every timestep to affect the output.  \n",
    "\n",
    "  * The main disadvantage of a bidirectional RNN is that you can't efficiently stream predictions as words are being added to the end.\n",
    "\n",
    "1. After the RNN has converted the sequence to a single vector the two `layers.Dense` do some final processing, and convert from this vector representation to a single logit as the classification output. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4fodCI7soQi"
   },
   "source": [
    "The code to implement this is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T02:22:23.832495Z",
     "iopub.status.busy": "2021-03-06T02:22:23.827052Z",
     "iopub.status.idle": "2021-03-06T02:22:23.865444Z",
     "shell.execute_reply": "2021-03-06T02:22:23.864938Z"
    },
    "id": "LwfoBkmRYcP3"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIGmIGkkouUb"
   },
   "source": [
    "Please note that Keras sequential model is used here since all the layers in the model only have single input and produce single output. In case you want to use stateful RNN layer, you might want to build your model with Keras functional API or model subclassing so that you can retrieve and reuse the RNN layer states. Please check [Keras RNN guide](https://www.tensorflow.org/guide/keras/rnn#rnn_state_reuse) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kF-PsCk1LwjY"
   },
   "source": [
    "The embedding layer [uses masking](../../guide/keras/masking_and_padding) to handle the varying sequence-lengths. All the layers after the `Embedding` support masking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T02:22:23.869986Z",
     "iopub.status.busy": "2021-03-06T02:22:23.869312Z",
     "iopub.status.idle": "2021-03-06T02:22:23.871520Z",
     "shell.execute_reply": "2021-03-06T02:22:23.871890Z"
    },
    "id": "87a8-CwfKebw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "print([layer.supports_masking for layer in model.layers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZlS0iaUIWLpI"
   },
   "source": [
    "To confirm that this works as expected, evaluate a sentence twice. First, alone so there's no padding to mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T02:22:23.876572Z",
     "iopub.status.busy": "2021-03-06T02:22:23.875957Z",
     "iopub.status.idle": "2021-03-06T02:22:39.071344Z",
     "shell.execute_reply": "2021-03-06T02:22:39.071808Z"
    },
    "id": "O41gw3KfWHus"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00195633]\n"
     ]
    }
   ],
   "source": [
    "# predict on a sample text without padding.\n",
    "\n",
    "sample_text = ('The movie was cool. The animation and the graphics '\n",
    "               'were out of this world. I would recommend this movie.')\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0VQmGnEWcuz"
   },
   "source": [
    "Now, evaluate it again in a batch with a longer sentence. The result should be identical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T02:22:39.077245Z",
     "iopub.status.busy": "2021-03-06T02:22:39.076627Z",
     "iopub.status.idle": "2021-03-06T02:22:39.234383Z",
     "shell.execute_reply": "2021-03-06T02:22:39.233811Z"
    },
    "id": "UIgpuTeFNDzq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00195633]\n"
     ]
    }
   ],
   "source": [
    "# predict on a sample text with padding\n",
    "\n",
    "padding = \"the \" * 2000\n",
    "predictions = model.predict(np.array([sample_text, padding]))\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRI776ZcH3Tf"
   },
   "source": [
    "Compile the Keras model to configure the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T02:22:39.249492Z",
     "iopub.status.busy": "2021-03-06T02:22:39.248844Z",
     "iopub.status.idle": "2021-03-06T02:22:39.260269Z",
     "shell.execute_reply": "2021-03-06T02:22:39.259803Z"
    },
    "id": "kj2xei41YZjC"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIwH3nto596k"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T02:22:39.264666Z",
     "iopub.status.busy": "2021-03-06T02:22:39.264047Z",
     "iopub.status.idle": "2021-03-06T02:28:25.734910Z",
     "shell.execute_reply": "2021-03-06T02:28:25.735353Z"
    },
    "id": "hw86wWS4YgR2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "391/391 [==============================] - 196s 491ms/step - loss: 0.6834 - accuracy: 0.5187 - val_loss: 0.5150 - val_accuracy: 0.7719\n",
      "Epoch 2/10\n",
      " 53/391 [===>..........................] - ETA: 2:43 - loss: 0.5166 - accuracy: 0.7696"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs=10,\n",
    "                    validation_data=test_dataset, \n",
    "                    validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T02:28:25.740115Z",
     "iopub.status.busy": "2021-03-06T02:28:25.739488Z",
     "iopub.status.idle": "2021-03-06T02:28:41.517183Z",
     "shell.execute_reply": "2021-03-06T02:28:41.517592Z"
    },
    "id": "BaNbXi43YgUT"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T02:28:41.536193Z",
     "iopub.status.busy": "2021-03-06T02:28:41.534526Z",
     "iopub.status.idle": "2021-03-06T02:28:41.788707Z",
     "shell.execute_reply": "2021-03-06T02:28:41.789209Z"
    },
    "id": "OZmwt_mzaQJk"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(1,2,1)\n",
    "plot_graphs(history, 'accuracy')\n",
    "plt.ylim(None,1)\n",
    "plt.subplot(1,2,2)\n",
    "plot_graphs(history, 'loss')\n",
    "plt.ylim(0,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DwSE_386uhxD"
   },
   "source": [
    "Run a prediction on a new sentence:\n",
    "\n",
    "If the prediction is >= 0.0, it is positive else it is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T02:28:41.793886Z",
     "iopub.status.busy": "2021-03-06T02:28:41.793276Z",
     "iopub.status.idle": "2021-03-06T02:28:43.578505Z",
     "shell.execute_reply": "2021-03-06T02:28:43.578914Z"
    },
    "id": "ZXgfQSgRW6zU"
   },
   "outputs": [],
   "source": [
    "sample_text = ('The movie was cool. The animation and the graphics '\n",
    "               'were out of this world. I would recommend this movie.')\n",
    "predictions = model.predict(np.array([sample_text]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7g1evcaRpTKm"
   },
   "source": [
    "## Stack two or more LSTM layers\n",
    "\n",
    "Keras recurrent layers have two available modes that are controlled by the `return_sequences` constructor argument:\n",
    "\n",
    "* If `False` it returns only the last output for each input sequence (a 2D tensor of shape (batch_size, output_features)). This is the default, used in the previous model.\n",
    "\n",
    "* If `True` the full sequences of successive outputs for each timestep is returned (a 3D tensor of shape `(batch_size, timesteps, output_features)`).\n",
    "\n",
    "Here is what the flow of information looks like with `return_sequences=True`:\n",
    "\n",
    "![layered_bidirectional](images/layered_bidirectional.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbSClCrG1z8l"
   },
   "source": [
    "The interesting thing about using an `RNN` with `return_sequences=True` is that the output still has 3-axes, like the input, so it can be passed to another RNN layer, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T02:28:43.584781Z",
     "iopub.status.busy": "2021-03-06T02:28:43.584193Z",
     "iopub.status.idle": "2021-03-06T02:28:43.614549Z",
     "shell.execute_reply": "2021-03-06T02:28:43.614929Z"
    },
    "id": "jo1jjO3vn0jo"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 64, mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T02:28:43.623287Z",
     "iopub.status.busy": "2021-03-06T02:28:43.622682Z",
     "iopub.status.idle": "2021-03-06T02:28:43.626478Z",
     "shell.execute_reply": "2021-03-06T02:28:43.626863Z"
    },
    "id": "hEPV5jVGp-is"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T02:28:43.631149Z",
     "iopub.status.busy": "2021-03-06T02:28:43.630534Z",
     "iopub.status.idle": "2021-03-06T02:38:32.675594Z",
     "shell.execute_reply": "2021-03-06T02:38:32.676009Z"
    },
    "id": "LeSE-YjdqAeN"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, epochs=10,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T02:38:32.680627Z",
     "iopub.status.busy": "2021-03-06T02:38:32.680025Z",
     "iopub.status.idle": "2021-03-06T02:38:58.223126Z",
     "shell.execute_reply": "2021-03-06T02:38:58.222698Z"
    },
    "id": "_LdwilM1qPM3"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T02:38:58.227551Z",
     "iopub.status.busy": "2021-03-06T02:38:58.226941Z",
     "iopub.status.idle": "2021-03-06T02:39:01.766208Z",
     "shell.execute_reply": "2021-03-06T02:39:01.765750Z"
    },
    "id": "ykUKnAoqbycW"
   },
   "outputs": [],
   "source": [
    "# predict on a sample text without padding.\n",
    "\n",
    "sample_text = ('The movie was not good. The animation and the graphics '\n",
    "                    'were terrible. I would not recommend this movie.')\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T02:39:01.813034Z",
     "iopub.status.busy": "2021-03-06T02:39:01.798624Z",
     "iopub.status.idle": "2021-03-06T02:39:02.029776Z",
     "shell.execute_reply": "2021-03-06T02:39:02.030155Z"
    },
    "id": "_YYub0EDtwCu"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "plt.subplot(1,2,1)\n",
    "plot_graphs(history, 'accuracy')\n",
    "plt.subplot(1,2,2)\n",
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xvpE3BaGw_V"
   },
   "source": [
    "Check out other existing recurrent layers such as [GRU layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU).\n",
    "\n",
    "If you're interestied in building custom RNNs, see the [Keras RNN Guide](../../guide/keras/rnn.ipynb).\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "text_classification_rnn.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
